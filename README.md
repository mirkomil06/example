# PromptViT-Percepta

[![Python](https://img.shields.io/badge/Python-3.13-blue.svg)]()
[![PyTorch](https://img.shields.io/badge/Framework-PyTorch-red.svg)]()
[![License](https://img.shields.io/badge/License-Academic%20Use-green.svg)]()
[![Status](https://img.shields.io/badge/Status-In%20Progress-yellow.svg)]()

Prompt-Tuned Vision Transformers (ViTs) for Explainable Fine-Grained Recognition.

This project explores prompt-tuned Vision Transformers for fine-grained image classification (birds, cars, flowers) with a focus on model **interpretability** using attention-based visual explanations.

> Course: Computer Vision (Fall 2025), Central Asian University  
> Team: Percepta

---

# ğŸ“‘ Table of Contents
1. [Project Overview](./README.md#-project-overview)  
2. [Objectives](./README.md#-objectives)  
3. [Quickstart](./README.md#-quickstart)  
4. [Repository Structure](./README.md#-repository-structure)  
5. [Baseline Results (Week 3)](./README.md#-baseline-results-week-3)  
6. [Datasets Used](./README.md#-datasets-used)  
7. [Methodology](./README.md#-methodology)  
8. [Project Roadmap](./README.md#-project-roadmap)  
9. [Team](./README.md#-team-percepta)  
10. [Tech Stack](./README.md#-tech-stack)  
11. [Ethics & Compliance](./README.md#-ethics--compliance)  
12. [Expected Outcomes](./README.md#-expected-outcomes)  
13. [Experiments & Evaluation](./README.md#-experiments--evaluation)  
14. [References](./README.md#-references)  
15. [License](#-license)  
16. [Repository Link](./README.md#-repository-link)

---

## ğŸ§  Project Overview

Fine-grained image classification deals with categories that look visually similar (e.g., bird species, car models, flower types). Traditional models achieve good accuracy but often lack **explainability** â€” itâ€™s difficult to understand *why* the model makes a specific prediction.

This project combines:

- **Vision Transformers (ViT-B/16)** â€” as the baseline model  
- **Visual Prompt Tuning (VPT)** â€” lightweight parameter-efficient adaptation  
- **Prompt-CAM & attention rollout** â€” to visualize what the model focuses on  
- **Evaluation metrics** â€” accuracy, F1-score, and pointing-game interpretability score  

The goal is to build a system that is **both accurate and explainable**.

---

## ğŸ¯ Objectives

- Build a baseline ViT-B/16 model for fine-grained recognition  
- Implement prompt-tuning (VPT-Deep / VPT-Shallow) to reduce trainable parameters  
- Add explainability methods (Prompt-CAM, attention rollout)  
- Evaluate trade-offs between full fine-tuning and prompt-tuning  
- Produce visual explanations that show why the model predicts a specific class  

---

## ğŸš€ Quickstart

Follow these steps to set up and run the project.

### 1. Clone the repository
```bash
git clone https://github.com/mirkomil06/PromptViT-Percepta.git
cd PromptViT-Percepta
```

### 2. (Optional) Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Download datasets
Download datasets from Kaggle and place them under data/ in this structure:
```bash
data/
â”œâ”€â”€ cub200/
â”‚   â””â”€â”€ CUB_200_2011/...
â”œâ”€â”€ cars/
â”‚   â”œâ”€â”€ car_devkit/
â”‚   â”œâ”€â”€ cars_train/
â”‚   â””â”€â”€ cars_test/
â””â”€â”€ flowers/
    â””â”€â”€ dataset/
        â”œâ”€â”€ train/
        â”œâ”€â”€ valid/
        â””â”€â”€ test/
```

### 5. Train the baseline model (CUB-200)
```bash
python -m src.scripts.train_cub_baseline --config src/configs/cub_baseline.yaml
```

### 6. Run inference on a single image
```bash
python -m src.scripts.infer_cub_baseline --image "data/cub200/CUB_200_2011/images/...jpg"
```
This will output the predicted class and confidence score.

---

## ğŸ“‚ Repository Structure

```bash
PromptViT-Percepta/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ ROADMAP.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ References/
â”‚    â”œâ”€â”€ AN IMAGE IS WORTH 16X16 WORDS.pdf
â”‚    â”œâ”€â”€ Visual Prompt Tuning.pdf
â”‚    â”œâ”€â”€ Learning Deep Features for Discriminative Localization.pdf
â”‚    â”œâ”€â”€ Transformer Interpretability.pdf
â”‚    â””â”€â”€ Literature_Review_Summary.md
â”‚
â”œâ”€â”€ data/ # datasets (not included due to size)
â”‚
â”œâ”€â”€ results/
â”‚    â””â”€â”€ cub_baseline_cpu.txt
â”‚
â”œâ”€â”€ outputs/
â”‚    â””â”€â”€ cub_baseline_cpu/
â”‚        â””â”€â”€ best_model.pth
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ configs/
    â”‚    â””â”€â”€ cub_baseline.yaml
    â”œâ”€â”€ datasets/
    â”‚    â””â”€â”€ cub200.py
    â”œâ”€â”€ models/
    â”‚    â””â”€â”€ vit_baseline.py
    â”œâ”€â”€ training/
    â”‚    â””â”€â”€ trainer_baseline.py
    â””â”€â”€ scripts/
         â”œâ”€â”€ train_cub_baseline.py
         â””â”€â”€ infer_cub_baseline.py
```

---

## ğŸ“Š Baseline Results (Week 3)

**Model:** ViT-B/16 (timm, pretrained on ImageNet-21k â†’ 1k)  
**Training Device:** CPU  
**Epochs:** 5  
**Dataset:** CUB-200-2011  

| Epoch | Validation Accuracy |
|-------|----------------------|
| 1 | 1.40% |
| 2 | 3.94% |
| 3 | 6.83% |
| 4 | 10.10% |
| 5 | **13.62%** |

**Best Validation Accuracy:** 13.62%  
**Checkpoint Saved:** outputs/cub_baseline_cpu/best_model.pth

---

### ğŸ“ Notes
- Low accuracy is expected because:
  - training was done **only for 5 epochs**
  - training was on **CPU**
  - ViT-B/16 is a **large model (~86M params)**  
- With more epochs + GPU, accuracy will improve significantly.

---

## ğŸ“š Datasets Used

We use three widely adopted fine-grained visual classification datasets:

| Dataset | Classes | Images | Description |
|---------|---------|---------|-------------|
| **[CUB-200-2011](https://www.kaggle.com/datasets/wenewone/cub2002011)** | 200 | 11,788 | Bird species with highly subtle inter-class variations; main dataset for baseline training |
| **[Stanford Cars](https://www.kaggle.com/datasets/eduardo4jesus/stanford-cars-dataset)** | 196 | 16,185 | Fine-grained car model classification (make, year, style) |
| **[Oxford Flowers-102](https://www.kaggle.com/datasets/nunenuh/pytorch-challange-flower-dataset)** | 102 | 8,189 | Flower species with strong visual similarity between categories |

These datasets are ideal for testing both **model accuracy** and **explainability**, since many classes are visually difficult to distinguish.

---

## ğŸ› ï¸ Methodology

Our pipeline consists of three major stages:

### **1. Baseline Vision Transformer (ViT-B/16)**  
- We fine-tune a pretrained **ViT-B/16** model using the `timm` library.  
- Only the classification head is replaced (1000 â†’ 200 classes).  
- This baseline is used as the reference point for later prompt-tuned models.  
- Evaluation: **Accuracy** and **F1-score**.

### **2. Prompt-Tuning (VPT-Deep / VPT-Shallow)**  
*Planned for Weeks 4â€“5.*

We integrate **Visual Prompt Tuning (VPT)** â€” a parameter-efficient alternative to full fine-tuning:

- The ViT backbone is **frozen**  
- Learnable **prompt tokens** are prepended to transformer inputs  
- Trainable parameters reduce from ~86M â†’ **<1%**  
- Expected benefits:  
  - Smaller memory footprint  
  - Faster training  
  - Comparable accuracy to full fine-tuning  
  - Better generalization on limited data

### **3. Explainability (Prompt-CAM & Attention Rollout)**  
*Planned for Weeks 5â€“6.*

To visualize what the model attends to:

- **Prompt-CAM** â€” class activation mapping adapted for prompt-tuned ViTs  
- **Attention rollout** â€” averages attention across layers  
- **Pointing Game metric** â€” evaluates how well the heatmap highlights the correct object  

Goal: Provide **faithful, human-understandable** explanations of model predictions.

---

## ğŸ“… Project Roadmap

| Week | Dates | Milestone | Status |
|------|--------|-----------|---------|
| **Week 1** | Oct 14â€“21 | Repo setup, topic confirmation | âœ… Completed |
| **Week 2** | Oct 21â€“27 | Literature review + dataset preparation | âœ… Completed |
| **Week 3** | Oct 28â€“Nov 3 | Baseline ViT-B/16 training on CUB-200 | âœ… Completed |
| **Week 4** | Nov 4â€“10 | Prompt-tuning implementation (VPT-Deep / VPT-Shallow) | â³ In progress |
| **Week 5** | Nov 11â€“17 | Explainability module (Prompt-CAM, attention rollout) | â³ Upcoming |
| **Week 6** | Nov 18â€“24 | Evaluation (Accuracy, F1, Pointing Game) | â³ Upcoming |
| **Week 7** | Nov 25â€“Dec 1 | Report writing & presentation slides | â³ Upcoming |
| **Week 8** | Dec 2â€“8 | Final cleanup & project presentation | â³ Upcoming |

ğŸ—‚ï¸ [**ROADMAP.md** file will include weekly progress updates and issue tracking.](./ROADMAP.md)

### ğŸ”„ Progress Summary

- Week 1â€“3: **Core pipeline completed**  
- Week 4: **Prompt-tuning implementation ongoing**  
- Future weeks: explainability â†’ evaluation â†’ report â†’ presentation

---

## ğŸ‘¥ Team Percepta

| Name | Role | Email |
|------|------|-------|
| **Mirkomil Mirzohidov** | Model architecture & repository management | 221408@centralasian.uz |
| **Muhammad Saidahmetov** | Experiments, evaluation metrics, prompt-tuning | 220838@centralasian.uz |
| **Asilbek Tashpulatov** | Dataset preparation, documentation & report writing | 221443@centralasian.uz |

We work together to build an explainable and efficient Vision Transformerâ€“based system for fine-grained classification.

---

## ğŸ› ï¸ Tech Stack

- **Python 3.13**
- **PyTorch** â€” deep learning framework  
- **timm** â€” Vision Transformer (ViT-B/16) implementation  
- **Pillow / torchvision** â€” image loading & preprocessing  
- **Matplotlib** â€” visualizations and plots  
- **tqdm** â€” progress bars for training  
- **Visual Studio Code** â€” main development environment  

---

## âš–ï¸ Ethics & Compliance

- All datasets used in this project (**CUB-200-2011**, **Stanford Cars**, **Oxford Flowers-102**) are publicly available and intended for academic research.
- The project does **not collect**, **store**, or **process** any personal or sensitive information.
- All model outputs and visualizations are used strictly for educational and research purposes.
- The code and methods follow standard practices in the machine learning and computer vision community.
- All referenced papers and datasets are cited and credited to their original authors.

---

## ğŸ“ˆ Expected Outcomes

By the end of the project, we aim to deliver:

- A fully trained **baseline ViT-B/16** model on fine-grained datasets  
- A **prompt-tuned Vision Transformer** (VPT-Deep / VPT-Shallow) with <1% trainable parameters  
- Explainability visualizations using **Prompt-CAM** and **attention rollout**  
- Evaluation metrics:
  - Accuracy  
  - F1-score  
  - Pointing Game (interpretability metric)  
- A clean and reproducible codebase with clear configuration files  
- A final **PDF report** and a **presentation** summarizing the project workflow and results  

---

## ğŸ”¬ Experiments & Evaluation

Our experiments are designed to compare three major components:

1. **Baseline ViT-B/16 fine-tuning**  
2. **Prompt-Tuned ViT (VPT-Deep / VPT-Shallow)**  
3. **Explainability quality (Prompt-CAM & attention rollout)**

### **1ï¸âƒ£ Experiment Setups**

| Experiment | Description | Status |
|-----------|-------------|--------|
| **E1 â€” Baseline ViT Training** | Full fine-tuning of ViT-B/16 on CUB-200 | âœ… Completed |
| **E2 â€” Prompt-Tuning (VPT-Deep)** | Add deep prompt tokens, freeze backbone | â³ In progress |
| **E3 â€” Prompt-Tuning (VPT-Shallow)** | Add prompts only to the first layer | â³ Planned |
| **E4 â€” Explainability Evaluation** | Generate Prompt-CAM & attention rollout | â³ Planned |
| **E5 â€” Pointing Game Metric** | Evaluate interpretability quality | â³ Planned |
| **E6 â€” Cross-dataset Generalization** | Evaluate CUB-trained model on Cars/Flowers | â³ Planned |

### **2ï¸âƒ£ Evaluation Metrics**

We evaluate models on both **accuracy** and **interpretability**:

#### **Classification Metrics**
- **Top-1 Accuracy**
- **F1-score**
- **Confusion Matrix**

#### **Interpretability Metrics**
- **Pointing Game** (localization accuracy)
- **CAM heatmap quality** (qualitative)
- **Attention Rollout visualization**

### **3ï¸âƒ£ Comparison Strategy**

We will compare:

| Model | Trainable Params | Expected Behavior |
|-------|------------------|------------------|
| **ViT-B/16 (full fine-tuning)** | ~86M | Highest accuracy, slow training |
| **VPT-Shallow** | <1% params | Lightweight, faster, stable |
| **VPT-Deep** | <1% params | Best for complex tasks |
| **No Prompts (frozen ViT)** | ~0 trainable | Weak baseline |

This comparison will show the **benefit of prompt tuning** vs full fine-tuning.

### **4ï¸âƒ£ Datasets for Evaluation**

- **CUB-200-2011** â†’ main dataset for all experiments  
- **Stanford Cars** â†’ cross-dataset generalization test  
- **Oxford Flowers-102** â†’ interpretability test (CAMs look very clear)

### **5ï¸âƒ£ Deliverables per Experiment**

Each experiment will produce:

- Training logs  
- Validation curves  
- Best checkpoint  
- Visual explainability maps  
- Metric comparison tables  

This ensures full reproducibility and clarity in final reporting.

---

## ğŸ§© References  

- Chefer H., Gur S., Wolf L. *Transformer interpretability beyond attention visualization.*  
  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 782â€“791.  

- Dosovitskiy A. *An image is worth 16x16 words: Transformers for image recognition at scale.*  
  arXiv preprint arXiv:2010.11929, 2020.  

- Jia M. et al. *Visual prompt tuning.*  
  In *European Conference on Computer Vision (ECCV)*. Cham: Springer Nature Switzerland, 2022, pp. 709â€“727.  

- Zhou B. et al. *Learning deep features for discriminative localization.*  
  Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 2921â€“2929.  

---

## ğŸ“œ License  
This project is conducted as part of the **Central Asian University â€” Computer Vision (Fall 2025)** course under academic fair use for research and educational purposes.

---

## ğŸŒ Repository Link  
[https://github.com/mirkomil06/PromptViT-Percepta](https://github.com/mirkomil06/PromptViT-Percepta)
